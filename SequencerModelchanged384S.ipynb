{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTMVertical:\n",
    "This is a module that implements a Bidirectional Long Short-Term Memory (BiLSTM) layer with a vertical direction of computation.\n",
    "It takes two arguments: input_size and hidden_size, which determine the input and hidden state dimensions of the LSTM.\n",
    "Inside the __init__ method, it initializes an nn.LSTM module with the specified input and hidden sizes. It's set to be bidirectional (bidirectional=True) and accepts input data in batches (batch_first=True).\n",
    "In the forward method, it applies the BiLSTM to the input tensor x and returns the output.\n",
    "BiLSTMHorizontal:\n",
    "This is another module similar to BiLSTMVertical, but it implements a BiLSTM layer with a horizontal direction of computation.\n",
    "It has the same constructor and forward method structure as BiLSTMVertical.\n",
    "FullyConnectedLayer:\n",
    "This module represents a fully connected (linear) layer in a neural network.\n",
    "It takes two arguments: input_size and output_size, determining the input and output dimensions of the linear layer.\n",
    "In the __init__ method, it initializes an nn.Linear module with the specified input and output sizes.\n",
    "In the forward method, it applies the linear transformation to the input tensor x and returns the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMVertical(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTMVertical, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "\n",
    "class BiLSTMHorizontal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTMHorizontal, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "\n",
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2D\n",
    "Is a sequencer in which the input data (images) are splitted into vertical adn horiziontal component, then vertical data will be passed as input to BilstmVertical ,while horizontal_data will be passed to BiLSTMHorizontal, to do so the input data are first processed in such a way we'll have a tensor with vertical data of each image and another tensor with horizontal data of each image. Oncee we get the outputs from both BiLSTMvertical and BiLSTMHorizontal, we 'll concatenate them and pass as input the result to Fully Connected Layer . At the end of the sequencer the results will be a list of tensor ( each tensor has inside data belongs to each image processed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sequencer2D(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, mlp_input_size, mlp_output_size):\n",
    "        super(Sequencer2D, self).__init__()\n",
    "\n",
    "       \n",
    "        vertical_bilstm = BiLSTMVertical(input_size, hidden_size)\n",
    "        horizontal_bilstm = BiLSTMHorizontal(input_size, hidden_size)\n",
    "        fully_connected = FullyConnectedLayer( mlp_input_size,mlp_output_size)  \n",
    "\n",
    "        # Move models to GPU if available\n",
    "        #vertical_bilstm.to(device)\n",
    "        #horizontal_bilstm.to(device)\n",
    "        #fully_connected.to(device)\n",
    "\n",
    "        self.vertical_bilstm = vertical_bilstm\n",
    "        self.horizontal_bilstm = horizontal_bilstm\n",
    "        self.fully_connected = fully_connected\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x)\n",
    "        if len(x.shape)==4:\n",
    "            batch_size,channel, height, width = x.size()\n",
    "            x=x.squeeze(1)\n",
    "        elif len(x.shape) == 3 :\n",
    "            batch_size, height, width = x.size()\n",
    "        output=[]\n",
    "        for index in range(batch_size):\n",
    "            x_vertical = x[index][:, :height//2]  #split the image vertically\n",
    "            x_horizontal = x[index][ :,width//2:]   #split image horizontally\n",
    "\n",
    "            vertical_data = x_vertical.permute( 1,0) #permutation to fit bilstm\n",
    "            horizontal_data = x_horizontal.permute( 1,0)#same here\n",
    "            \n",
    "            \n",
    "            \n",
    "            H_ver = self.vertical_bilstm(vertical_data)\n",
    "            H_hor = self.horizontal_bilstm(horizontal_data)\n",
    "            \n",
    "            \n",
    "            H_ver = torch.tensor(H_ver, dtype=torch.float32).permute(1,0) #permute to fit torch.cat\n",
    "            H_hor = torch.tensor(H_hor, dtype=torch.float32).permute(1,0)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            H_concat = torch.cat((H_ver, H_hor), dim=1)#concatenaiton of the horizontal and vertical bilstm outputs\n",
    "            H_concat = H_concat.unsqueeze(0) #set dimension to fit fully conncted layer\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            output_pieces = self.fully_connected(H_concat)\n",
    "            \n",
    "            output_pieces=torch.tensor(output_pieces, dtype=torch.float32)\n",
    "            output.append(output_pieces) #add output to lists of each image data processed here\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Embedding : this costum function given an image , a patch size, will splitted the image in image_size/patch_size pieces.\n",
    "\n",
    "image_size: The size of the input image. It represents both the height and width of the image.\n",
    "patch_size: The size of each non-overlapping patch that the image is divided into.\n",
    "in_channels: The number of input channels in the image. For RGB images, this is typically 3 (for the red, green, and blue channels).\n",
    "embed_dim: The dimension of the embedded representations for each patch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        \n",
    "        self.num_patches = (image_size // patch_size) ** 2 #compute the number of patches in such a way they won't overlap\n",
    "\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) #convolutional layer takes the input image, divides it into patches, and embeds each patch into a lower-dimensional representation.\n",
    "\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.reshape(B, self.num_patches, C, -1).permute(0, 2, 1, 3)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def output_dimension(self):\n",
    "        return self.embed_dim * self.num_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PW Linear Layer compute  pointwise linear transformations on a list of input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(PWLinearLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, input_list):\n",
    "        output_list = []\n",
    "        for input_tensor in input_list:\n",
    "            output_tensor = self.linear(input_tensor)\n",
    "            output_list.append(output_tensor)\n",
    "        stacked_output = torch.stack(output_list, dim=0)\n",
    "        return stacked_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch merging, take the images splitted in patches (for examples by patch embedding ) and merge it together, the number of patches merged is based on the output channlee and  the  scale factor number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, patch_list):\n",
    "        \n",
    "        x = torch.cat(patch_list, dim=1).permute(1,2,0) # compination of the list of patches into a single tensor + peermutation to fit convolutional layer\n",
    "        \n",
    "        \n",
    "        x = self.conv(x) # Apply to the input x, a 1x1 convolution to merge the patches together\n",
    "        \n",
    "        \n",
    "        x = nn.functional.interpolate(x, scale_factor=self.scale_factor, mode='nearest') #resize the feature map using a nearest-neighbor upsampling\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global avarage pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAveragePooling, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.mean(x, dim=(-2, -1))# perform global average pooling along spatial dimensions using the mean\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2Dmodel-> Is an architecture that resambles vision transformer except for the fact that Sequencer2D model is using BiLSTM2D (splitting image in horizzontal and vertical data) instead of self attention. It started with patch embedding layeer so the images will be divided in 8x8 patches, than ,after a normalization, there is the first sequence block of 4 Sequencer , the output of this block converge in PatchMerging, where the patches are resembled together ,then there is a second sequencer block made of 3 sequencer2d, followed by pointwise linear layer,then other sequencer block with 8 sequencers2D, then other point wise linear layer and  the final sequencerBlock with 3 sequencer2D layer and at the end there are linears+ global avarage pooling layers in order to make the output dimension as expected-\n",
    "\n",
    "The parameters are the one suggested by the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sequencer2DModel(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super(Sequencer2DModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "\n",
    "       \n",
    "        self.patch_embedding_1 = PatchEmbedding(32, 8, in_channels, 16)#  patch embedding with an 8x8 kernel size for each patch\n",
    "        self.ln_1 = nn.LayerNorm(16)\n",
    "\n",
    "       \n",
    "        self.sequencer_block_1 =  nn.Sequential(\n",
    "            Sequencer2D(16, 48, 6, 96),\n",
    "            Sequencer2D(96, 96, 96, 192),\n",
    "            Sequencer2D(192, 192, 192,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "\n",
    "\n",
    "       \n",
    "        self.patch_merging=PatchMerging(24576,64,2)#it still depend on batch size!!! it is sooo wrong\n",
    "\n",
    "      \n",
    "        self.sequencer_block_2 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 3,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "\n",
    "       \n",
    "        self.pw_linear_1 = PWLinearLayer( 384,384)\n",
    "        \n",
    "        \n",
    "        self.sequencer_block_3 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            \n",
    "            \n",
    "           \n",
    "        )\n",
    "\n",
    "        self.pw_linear_2 = PWLinearLayer(384, 384)\n",
    "\n",
    "        \n",
    "        self.sequencer_block_4 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "        self.pw_linear_3 = PWLinearLayer(384, 384)\n",
    "\n",
    "\n",
    "        \n",
    "        self.ln_2 = nn.LayerNorm(384)\n",
    "\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 384))\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(384, num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        \n",
    "        x = x.to(torch.float32).permute(0,3,1,2)  \n",
    "        x = self.patch_embedding_1(x)\n",
    "        x = self.ln_1(x)\n",
    "\n",
    "        \n",
    "        x = self.sequencer_block_1(x)\n",
    "        \n",
    "        print(len(x))\n",
    "        \n",
    "        x = self.patch_merging(x)\n",
    "        \n",
    "        \n",
    "        x = self.sequencer_block_2(x)\n",
    "\n",
    "        \n",
    "        x = self.pw_linear_1(x)\n",
    "        \n",
    "        x = self.sequencer_block_3(x)\n",
    "        \n",
    "        \n",
    "        x = self.pw_linear_2(x)\n",
    "\n",
    "    \n",
    "        x = self.sequencer_block_4(x)\n",
    "        \n",
    "        x = self.pw_linear_3(x)\n",
    "\n",
    "        \n",
    "        x = self.ln_2(x)\n",
    "        \n",
    "        x = self.global_avg_pool(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomCifar2, this custom function process imace into the dataset in order to take only the first five classes, I've tried this apporch because each batch processing was too slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomCIFAR2(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(CustomCIFAR2, self).__init__()\n",
    "        self.cifar10 = datasets.CIFAR10(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "        \n",
    "        \n",
    "        self.keep_classes = [0, 1,2,3,4] \n",
    "\n",
    "        \n",
    "        self.data, self.targets = self.filter_classes()\n",
    "\n",
    "    def filter_classes(self):\n",
    "        mask = np.isin(self.cifar10.targets, self.keep_classes)\n",
    "        data = [self.cifar10.data[i] for i, include in enumerate(mask) if include]\n",
    "        targets = [self.cifar10.targets[i] for i, include in enumerate(mask) if include]\n",
    "        return data, targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model, with trnsform.Compose function that helps me with preprocessing and data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = Sequencer2DModel(num_classes=5, in_channels=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(15),      # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.8),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),# Adjust brightness, contrast, saturation, and hue\n",
    "    transforms.RandomResizedCrop(16),\n",
    "    transforms.RandomResizedCrop(4),# Randomly crop and resize the image to 224x224\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomAffine(degrees=4, translate=(0.4, 0.1)),# Randomly translate the image\n",
    "    transforms.ToTensor(),              # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "#train_dataset = torchvision.datasets.CIFAR10(root='./cifar-10-batches-py', train=True, transform=transform) #use this for complete dataset\n",
    "custom_dataset = CustomCIFAR2(root='root='./cifar-10-batches-py', train=True, transform=transform, download=True)\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part the code aims to train the model: it iterates over  the number of epochs and then over all the batchs in the datasets takes the images and the labels and pass it to the model ,get the model outputs, and reshape it in order to make it fit well the loss function(CrossEntropyLoss function , 'cause it is a multiclassification problem), then performs backward step, and calls the optimizer (Adam starting with 0.01 learning rate) ,then it takes the prediction and compere it with the true labels,after that the accuracy is computed simply by divided the true presiction to the total  number of images processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x3072 and 96x6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ae2737a2e5c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-00ccd47333ef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequencer_block_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-0eeac8aa39d7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Apply MLP to the concatenated output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmlp_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmlp_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x3072 and 96x6)"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "all_labels=[]\n",
    "all_accuracy=[]\n",
    "all_loss=[]\n",
    "# Training phase\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    running_loss = 0.0  # track the loss for 200 batch\n",
    "    \n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        labels_=[]\n",
    "        if i==390: #jump one batchs due to  incompatibility \n",
    "            continue\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "       \n",
    "        optimizer.zero_grad() #reset the gradient\n",
    "        \n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        print(outputs.shape)\n",
    "        \n",
    "        outputs=outputs[:,0]\n",
    "        outputs=outputs[:,0]\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        all_loss.append(loss)\n",
    "      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "       \n",
    "        predicted_probabilities = outputs.argmax(dim=1)\n",
    "        #correct = (predicted_probabilities == labels).sum().item() #sum up all the right prediction\n",
    "        _,predicted_labels = torch.max(outputs,dim=1)\n",
    "        correct = (predicted_labels == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        \n",
    "        print(labels)\n",
    "        print(predicted_labels)\n",
    "\n",
    "        batch_accuracy = (correct / labels.size(0)) *100\n",
    "        all_accuracy.append(batch_accuracy)\n",
    "        for label in labels:\n",
    "            all_labels.append(labels)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss: {loss.item():.4f} Accuracy: {batch_accuracy:.2f}%\")\n",
    "        if i % 200 == 199:  \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss: {loss.item():.4f} Accuracy: {batch_accuracy:.2f}%\")\n",
    "            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_accuracy = (total_correct / total_samples) * 100.0\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-6bac0eae6cdf>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_loss=torch.tensor(all_loss)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASkAAAEWCAYAAAA6tWH6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4X0lEQVR4nO2deXhb9ZX3P8fyKq+Knc1WsBySkAVCQkISoOxtocNWKHQIpeWdt++09Hm7d2bKdEppp50ZpnRmKLQdylDaDlAo2xTa8tIVEmhxNkhCgh1IvMRO7HiR993Wef+Q5AjHtmRb0r2Sfp/n8YNzdX3vkbG//v3O/Z5zRFUxGAwGu5JmdQAGg8EwHUakDAaDrTEiZTAYbI0RKYPBYGuMSBkMBltjRMpgMNgaI1KGlEJEDorIJVbHYYgcI1JJiojUich7Lbr3+SLyRxHpEZEuEfmliKyO0717Qz58IjIQ8u+PqOoaVX05HrEYooMRKUNUEZHzgN8CzwGlQAWwD/iTiCyN8r1ERN71M6yqecEP4ChwTcixx6J5f0N8MCKVYohIlojcKyLHAx/3ikhW4LUSEfmViHSKiFdEXgmKgIh8WUSOBVZHh0Tk8ilu8W3gv1X1u6rao6peVf0qUAl8PXCtKhG5OiSmdBFpE5FzAv/eIiJ/DsSxL3R7JiIvi8g/icifgH5gRsIXusIUka+LyFMi8mjgfb0pIitE5O9FpEVEGkTk/SFfWygiPxKRpsD34lsi4pjJ/Q0zx4hU6vEPwBZgHXA2sAn4auC1LwGNwHxgIfAVQEXkDODTwLmqmg9cAdRNvLCIOIHzgacmue+TwPsCnz8ObA157QqgTVVfF5Ey4NfAt4B5wN8Az4jI/JDzPwp8AsgH6iN/65NyDfAI4ALeAH6D//eiDPhH4Ich5/4UGAWWAeuB9wP/Z473N4TBiFTq8RHgH1W1RVVbgW/g/6UHGAEWA+WqOqKqr6i/uHMMyAJWi0iGqtap6pFJrj0P/89U0ySvNQElgc9/BlwbEDWAWwLHAG4FXlDVF1TVp6q/A3YDfxFyrZ+o6kFVHVXVkVl8D0J5RVV/o6qj+MV1PnB34LpPAB4RKRKRhcAHgM+rap+qtgD/Adw8x/sbwmBEKvUo5d2rj/rAMYB7gMPAb0WkRkTuAFDVw8Dn8W/XWkTkCREp5VQ6AB9+oZvIYqAt5HpVwDUBobqWkyJVDtwU2Op1ikgn8J4J12yYyRsOw4mQzwfwr+jGQv4NkBeIKwNoConrh8CCKMZimAQjUqnHcfy/cEFOCxwjkEP6kqouxb8N+mIw96SqP1PV9wS+VoF/nXhhVe0DXgNumuS+Hwb+EPLv4JbvOuCtgHCBX4AeUdWikI9cVb079FYzftdzpwEYAkpC4ipQ1TUWxJJSGJFKbjJEJDvkIx2/OHxVROaLSAnwNeBRABG5WkSWiYgA3fi3eWMicoaIXBZIsA/iX2GMTX5L7gBuE5HPiki+iLhE5FvAefi3lkGewJ/T+RQnV1EEYrlGRK4QEUcg7ktExB2tb8psUNUm/E8t/01ECkQkTUROF5GLrYwrFTAildy8gF9Qgh9fx5+Q3g3sB94EXg8cA1gO/B7oxb8i+kHAU5QF3I1/u9aMf4vzlcluqKqv4k+E34A/D1WPP8n8HlV9J+S8psA9zgd+HnK8Af/q6itAK/4VzN9ij5/VjwGZwFv4t7ZPM/nW1hBFxDS9MxgMdsYOf50MBoNhSoxIGQwGW2NEymAw2BojUgaDwdakhztBRB4GrgZaVPXMSV7/W/wu5uD1VgHzVdUrIlcC3wUcwEMTvC5TUlJSoh6PJ7J3YDAYEp49e/a0qer8yV4L+3RPRC7C/0j6vycTqQnnXgN8QVUvCxRevo2/XqsR2AVsVdW3wgW8ceNG3b17d7jTDAZDkiAie1R142Svhd3uqep2wBvhvbbiNwuCv3D1sKrWqOowfvPedRFex2AwGIAo5qQCNVhXAs8EDpXx7hqrxsAxg8FgiJhoJs6vAf6kqsFVl0xyzpR7SxH5hIjsFpHdra2tUQzLYDAkMmET5zPgZk5u9cC/cloS8m83gULWyVDVB4EHwZ+Tmvj6yMgIjY2NDA4ORidam5GdnY3b7SYjI8PqUAwGWxEVkRKRQuBi/L2AguwClotIBXAMv4jdMtt7NDY2kp+fj8fjwV//mjyoKu3t7TQ2NlJRUWF1OAaDrYjEgvA4cAlQIiKNwF34++qgqg8ETrse+G2gVQeB10ZF5NP4Ox06gIdV9eBsAx0cHExKgQIQEYqLizHbXIPhVMKKlKpujeCcnwA/meT4C/gr8aNCMgpUkGR+bwbDXDCOc4MhBfH5lCd3N9A1MNfuy7HHiNQMyMvLszoEgyEqvHSohb97ej/P7z1mdShhMSJlMKQgj1T629zXtvVbHEl4jEjNkb1797JlyxbWrl3L9ddfT0dHBwD33Xcfq1evZu3atdx8s3+gyLZt21i3bh3r1q1j/fr19PT0WBm6IUWpb+9j29ut45/bnWj6pOLGN355kLeOd0f1mqtLC7jrmpn31P/Yxz7G/fffz8UXX8zXvvY1vvGNb3Dvvfdy9913U1tbS1ZWFp2dnQB85zvf4fvf/z4XXHABvb29ZGdnR/U9GAyR8NiOozhEWLukkNoEECmzkpoDXV1ddHZ2cvHF/l78t912G9u3bwdg7dq1fOQjH+HRRx8lPd3/t+CCCy7gi1/8Ivfddx+dnZ3jxw2GeDE4MsaTuxu4Ys0iNlUU0+DtZ8xn7xbiCflbMpsVT7z59a9/zfbt23n++ef55je/ycGDB7njjju46qqreOGFF9iyZQu///3vWblypdWhGlKI5/cdp7N/hFu3lFPf3sfImHK8c4Al85zhv9gizEpqDhQWFuJyuXjllVcAeOSRR7j44ovx+Xw0NDRw6aWX8u1vf5vOzk56e3s5cuQIZ511Fl/+8pfZuHEj1dXVFr8DQ6rxaGU9KxbmsWXpPMqLcwGob7d38jwhV1JW0d/fj9t9cvzbF7/4RX76059y++2309/fz9KlS/nxj3/M2NgYt956K11dXagqX/jCFygqKuLOO+/kpZdewuFwsHr1aj7wgQ9Y+G4Mqcbehk72N3bxzevWICJ4Svyrp9r2Pt6zvMTi6KbGiNQM8Pl8kx6vrKw85dirr756yrH7778/6jEZDJHyyGv15GY6+OB6f8ekhfnZZGekUd9m7+S52e4ZbMHbJ3q47eGd9A+PWh1KUuLtG+aX+49zwzlu8rP9nTbS0oTyebnU2Xy7Z0TKYAt+8cYxtr3dSnWz8Y7Fgid3NzA86uOj55W/63h5sdP2XqmEEqlknraczO8tEnbV+XslNnYMWBxJ8jHmUx7bUc/minmsWJj/rtc8JbnUe/vx2diGkDAilZ2dTXt7e1L+Mgf7SaWquXNwZIx9DV0AHDMiFXW2vd1Cg3fglFUUgKc4l+FRH03d9m0mmTCJc7fbTWNjY9L2XAp25kxF9jd2MTzmfyhxrNPe+ZFE5JHX6lmQn8UVaxad8pqn2P+Er76tj7KinHiHFhEJI1IZGRmma2WSEtzqlRXlmO1elDna3s/Lb7fy2cuWk+E4deNUXuL3StW193P+snhHFxkJI1KG5GVnrZflC/JYOj+XmlZ7J3ETjcd21JMmwtZNp036+uKCbDLT02ydPE+YnJQhORnzKa/Xd3BuxTzKipw0dgwkZd7RCgZHxvj57gauWLOQRYWT5zv9NgQntTb2ShmRMlhKVVM3PUOjbPLMw+3KYWBkjI5++3eLTAR+GVKnNx3lxbm2Lo0xImWwlN2BfNS5FfMoc/kTt+YJX3R4tLKe5QvyOG9p8bTneYqd1Hv7bGtDMCJlsJRddR2UFeWMfwA0dtj3r3qisK+hk32NXXz0vPKwQz7KS3IZHPHR0jMUp+hmhhEpg2WoKjvrvJzrcQGwxOV/HH6s06yk5sojlf46vesDdXrTURHohmDXvJQRKYNl1Lf309ozxLkV8wAoyEknLyvd2BDmSEffML/cd5zrzykbr9ObjvKgV8qmT/iMSBksY2cgH7XJ4xcpEcHtMl6pufLk7gaGRn18dIsnovNLi3LIcIhtC42NSBksY1etF5czg2ULTo4KKyvKMdu9OeDzKY/uqGdTxTzOWJQf/gsAR5qwZJ59C42NSBksY1edl42eee9K7Ja5ckzifA5se7vVX6cXxnYwkYriXJOTMhhCaekZpK69f3yrF8TtyqFncJTuQeOVmg2PVNYzf4o6vekIeqXsaKQ1ImWwhF21/vmEwaR5kLKiwBM+k5eaMQ3efl461MLWTaeRmT6zX21PiZOBkTFabWhDCPtORORhEWkRkQPTnHOJiOwVkYMisi3k+BcCxw6IyOMikpq9SAynsKvOS06GgzWlBe867nYFvVJGpGbKo4E6vVumqNObjuBQBjsmzyOR258AV071oogUAT8ArlXVNcBNgeNlwGeBjap6JuAAbp5jvIYkYWetl/WnFZ1SmX/SdW6/XxY7MzgyxpO7Gnj/6qnr9KYj6JWqs2FeKqxIqep2wDvNKbcAz6rq0cD5LSGvpQM5IpIOOIHjc4jVkCR0D45Q1dzNuRPyUQDFuZlkZ6SZldQM+dX+Jjr6R2acMA9SWpRNeppQZ8MnfNHISa0AXCLysojsEZGPAajqMeA7wFGgCehS1d9OdRER+YSI7BaR3cna2M7gZ099B6qwqeJUkRIRY0OYBY9U1rNsQR7nnT59nd5UpDvSAjYE+61goyFS6cAG4CrgCuBOEVkhIi7gOqACKAVyReTWqS6iqg+q6kZV3Th//vwohGWwK7tqvaSnCetPK5r09TKX04jUDNjf2Mm+hk4+uiV8nd50lBc7k3Yl1Qi8qKp9qtoGbAfOBt4L1Kpqq6qOAM8C50fhfoYEZ1edlzVlhTgzJ++5aFznM+OR1+pxZjq4/pzwdXrT4SnOpa6tz3Y2hGiI1HPAhSKSLiJOYDNQhX+bt0VEnOKX98sDxw0pTHDowqZAUfFklBXl4O0bNjP4IqCjb5jn9x3n+vVlFERQpzcdnmInfcNjtPUORym66BC2fbCIPA5cApSISCNwF5ABoKoPqGqViLwI7Ad8wEOqeiDwtU8DrwOjwBvAg7F4E4bEITh0YbKkeRB3SF+p5QsjK+1IVZ7aE6jTm2QSzEwJ9juvb+9jfn7WnK8XLcKKlKpujeCce4B7Jjl+F35RMxiAk0MXIhGpxk4jUtPh8ymPVh5lk2ceKxcVhP+CMHhCvFIbp/n/E2+M49wQV4JDF1y5mVOeY1znkbHtnVaOevu5NQqrKPD/cXCkie28UkakDHEjdOjCdCzIzyLDISZ5HoZHX6unJC+LK2dYpzcVGY403K4c2z3hMyJliBuhQxemIy1NKDVeqWlp8Pbzx0Mt3LJpyYzr9KbDjkMZjEgZ4saukKEL4fAPCrXXL4udeGzHUf88vc0zr9ObDk/AK2UnG4IRKUPc2FXnfdfAhelwu3JMTmoKBkfG+Pmuo7xv1UIWF0Z3NLqnOJeewVG8ffaxIRiRMsQFVWVnbcf40IVwlBU5aekZYmh0LMaRJR6/DtbpRSlhHoqnxP/Qwk7dEIxIGeJCXXs/bb1DEW314KQN4XjnYCzDSkgeqazn9Pm5nD/LOr3pCLZssVMrYSNShriwq/bdQxfCYQaFTs6bjV3sjUKd3lQscTlJE7OSMqQgO+tOHbowHWZQ6OQ8UlmHM9PBDRvcMbl+ZnoaZa4cW3mljEgZ4sJkQxemY3FhNo40MTaEEDr7h3lu73E+GIU6venwFOea7Z4htWjpHqR+kqEL05HuSGNRQbbZ7oXw1O7GwDy96CfMQ/G3bLHPCtaI1BT0DI6YrUaU2DkDf1QoZaZlyzjBeXrnelysWjz3Or3p8BTn0jUwQme/PWwIRqSm4NsvHuLq+19lcMQ8Ap8ru2onH7oQDrdxnY+z/Z1W6tv7uTXGqyg4WWhslzl8RqSmYF9jJ539I/zmYLPVoSQ8O+s6OKf81KEL4Shz5dDUNcDImC9GkSUOj1b66/Q+cObimN8r6JWyS3mMEalJGPMph5p7AHh6T6PF0SQ2XQMjVE8xdCEcblcOPoXmrtT2SjV4+/lDdQtbo1ynNxVulxMRbFNobERqEura+xga9eEpdvLq4TaOmy3HrHk9OHRhFiI13rIlxb//j+04igBbZzFPbzZkZzgoLcwxKyk7U93kX0V9+cqVqMKzr5vV1GzZWRccuhBZOUwoZlBoYJ7e7gbet3ohpRHUPEaL8mKnyUnZmaqmbhxpwqUrF7Bl6Tye3tNoq6rwRGJXrZczywrJyXTM+GsXF/mHXKayDeGFN5vw9g3z0S2euN7XU2Ifr5QRqUmobu5maUku2RkObtywhLr2fvbUd1gdVsIxODLG/sauSefrRUJWuoMF+VkpbQV5pLKepfNzuWBZ9Ov0psNT7KSjf4Su/pG43ncyjEhNQlVTDysDXpQPnLkIZ6aDp3abLd9M2dfQGXboQjjcrtS1Ibxzooc3jnbykc2xqdObjvFCY6/1qykjUhPoGhjhWOcAKxf5BwDkZqVz1VmL+fWbTWbE0gwJNrnbWD7zfFSQVB4U+qfDbQC8f/XCuN/bTl4pI1ITCFoPVi0+OaXkxg1ueodGefGA8UzNhJ11HaxYOP3QhXC4XTkc7xzA50u9nOCOWi9uVw5L5jnjfu/yYvt4pYxITaC6uRvgXaUHmyrmcdo8p/FMzYDxoQtzHI1UVpTDyJjS0jMUpcgSA59P2VHrZXNFfHNRQbIzHCwuzLaFV8qI1ASqmnoozMlgUUH2+DER4cYNbv58pD2lk7gzoaqpm96h0bmLlCs1W7a809KLt2+YLUutm39XXuw0Kyk7UtXUzcpF+ackKm84pwwReGbPMYsiSyxmMnRhOpYEm9+lWF6qsqYdgC1LrVlJgT8vZYe+UkakQvAFymEmqzJ3u5ycf3oxT7/ekJL5kZkyk6EL01FalJqGzh217ZQVWZOPCuIpyaW9b5juQWttCEakQjjq7WdgZOxdSfNQbtzgpsE7MN56xDA5Mx26MB3OzHSKczNTSqRUlcoaL5st3OqB3ysFcNTiLV9YkRKRh0WkRUQOTHPOJSKyV0QOisi2kONFIvK0iFSLSJWInBetwGNBVZM/ab5y0eQtRa5cs5i8rHSTQA/DTIcuhKMsxbxSJ/NR1m314KRXyurkeSQrqZ8AV071oogUAT8ArlXVNcBNIS9/F3hRVVcCZwNVs440DlQ195AmsGLh5CupnEwHV69dzAtvNtE3ZDxTUzHToQvhSLVBoTsC+ajzLBepwHgri/NSYUVKVbcD0+1vbgGeVdWjgfNbAESkALgI+FHg+LCqds414FhS3dSNpyR32jqzGze46R8e44U3m+IYWWIx06EL4Qh6pVKlfrKyxktpYfZ4gbVVODPTWViQZXkr4WjkpFYALhF5WUT2iMjHAseXAq3Aj0XkDRF5SERyp7qIiHxCRHaLyO7W1tYohDVzqpt7WDXFVi/IhnIXFSW5Zss3DTMduhCOsqIcBkd8tNtoqm6s8Oej2tmytDjupTCTUW6DoQzREKl0YANwFXAFcKeIrAgcPwf4T1VdD/QBd0x1EVV9UFU3qurG+fPnRyGsmdEzOMJRb/94OcxUBD1TO2q9licU7chshi6Ew+3ybztSIXl+uKWXdhvko4J4bDCUIRoi1Yg/79Snqm3Advz5p0agUVV3BM57Gr9o2ZK3TwTLYcL34Q56pp42faZOYbZDF6YjlQaFVgbyeXYRqfLiXFp7hui1MAcbDZF6DrhQRNJFxAlsBqpUtRloEJEzAuddDrwVhfvFhKpAo7uVU9gPQllcmMN7lpXwzJ5G45mawGyHLkxHKrnOK2vaWVyYzZJ51uajglSUWD92PRILwuPAa8AZItIoIh8XkdtF5HYAVa0CXgT2AzuBh1Q1aFf4DPCYiOwH1gH/HIP3EBWqmrrJz06P2Hx408YlHOscGHcGG/zMdujCdBRkZ1CQnZ70NgRVZYeN8lFgj0Lj9HAnqOrWCM65B7hnkuN7gY2ziizOBJPmkf5wvH/1QvKz/Z6p85eVxDi6xCA4dOFzly+P+rXLXM6k3+4dae2lrdfaer2J2MErZRznnCyHiWSrFyQ7w8E1Z5fywoEmeiwuG7ALcxm6EA53CgwKrazx56Os6nwwGXlZ6ZTkZVHfZt1KyogU/uLV3qHRKZ3mU3HTBjeDIz7jmQowl6EL4SgLDApNZq9UZU07iwqyx7dYdqGixEmtWUlZy1tNwR5Ska+kANYtKeL0+fbyTDV4+3li51FLfpnnMnQhHG5XDr1Do3QNJOeqNVivt2Vp9Pxl0cJqr5QRKfwjrGSacpipEBFu2riEXXUdtmizOjLm45OP7OGOZ9/kv1+rj+u95zp0IRzJPt6qpq2Ptt4h21gPQvEUOznRPWRZ+2wjUvi7cZbPc5KbFfY5wilcv76MNIFnbLCa+v5Lh3mrqZtlC/L4p19XcfB4V9zuHY2hC9OR7INCg0+JN9tQpILJ86Nea/JSRqTw2w8iMXFOxsKCbC5aMZ9nXm9kzELP1MHjXXzvj4e5bl0pP//EFoqcGXzm8Tfi9tcvGkMXpiPZV1KVNV4WFmSNt0exE0GvlFWFxikvUn1Do9R7+2ecNA/lpg1LaOoa5M9H2qIYWeQMj/r40pP7KHJm8vVr1lCcl8W9N6+jtq2Pu547GJcYojF0YTqKnBk4Mx1JaUOwW73eRE4LdkOwyCuV8iL19okeVCNzmk/F5asWUJiTYVkC/XsvHaa6uYd/vv7McZE4//QSPn3pMp7a08hze2Pb8jhaQxemQ0SStmVLbVsfrT1DtrIehFKQnUFxbqZlyfOUF6nq4AirOayksjMcXLeulBcPNMf96dOBY1384KXDfHBdKe9fs+hdr33u8uVsLHfxD/9zIKY/YMGhC7FKmgdJ1kGhQX+UnUycEykvdlJnkVcq5UWqqqmbvKz0OffuuXGDm6FRH7/eHz/P1PCoj795ah+u3Ey+fu2aU15Pd6Rx783rSBP4zONvMDzqi0kcOwNFsbFcSUHyduisrGlnQX7WeO7HjnhKci1znae8SFU39XDGonzS0uaWCzirrJAVC/N4ak9DlCILz/f++A7VzT38y/VnUeScPBfkdjn59o1r2d/YxXd+eygmcQSHLpTOcehCONwuJ539I5ZW5Ecbu+ejgniKc2nqGmRwZCzu905pkVJVqpq7w/aQigQR4aYNS3jjaCeHW3qjEN30HDjWxfdfPsIN68t4b5gx3FeeuZhbt5zGg9trePlQS1TjUFV21XljvtUDxou/kyl5XtfeT0vPkOVDF8IRdMFbYUNIaZE61jlAz+DorO0HE7lufSmONOGZGPeZCm7zinMzueuaU7d5k/HVq1azclE+X3pyHy3dg1GLpbatj7be4Zhv9SA5W7bYYb5eJHiKrbMhpLRIVTcFG93NfSUFsCA/m0vPmM+zMfZM3R/c5t1wFoXOjIi+JjvDwf1b19M3PMoXn9wXtT5YQX/UporY+KNCcSfhoNDKmnbm52ex1Mb5KAgRKQvyUqktUs3+mr0z5vBkbyI3bnBzonuIV96JTZ/2Nxu7+MHLR7jhnDIuXzX9Nm8iyxfm8/Vr1vDq4TYe2H4kKvHsrO1gXm4mp8+PztCF6SjJzSIzPS1ptnv+/lFe2+ejAAqdGbicGZZ4pVJapKqaejhtnpO8WZTDTMVlKxficmbwVAw8U0OjY/zNU/soycvkrqsj2+ZN5C/PXcJVaxfzb799mz31HXOOaVedl43lrrj8kqWlCe6i5GnZUt/eT3P3IJvjkM+LBlYVGqe2SEUpaR5KZnoa160r43cHT9DVH13P1H1/eIdDJ2a2zZuIiPAvN5zF4sJsPvv4G3PydZ3oHuSotz8uSfMgZa4cGpNku5co+aggHou8UikrUgPDY9S19bEySknzUG7c4GZ4zMfz+49H7Zr7Gzt5YFsNHzrHzWUrZ7bNm0hBdgb3bV3Pie5BvvLsm7Nu6xIvf1QoZUU5HEuSxHllTTsleVmcPt/e+aggnpJcjncNxN2GkLIi9U5LDz6FVVFeSQGcWVbIqsUFPL07Op6p0G3e165ZHZVrnnOaiy+9/wx+/WYTT+yaXZy76vxDF1ZHcehCONyuHNp6hy3x60QTVWVHrZfNNuwfNRWe4lxU4/90NWVFqmq80V1sfsFu3OBmX2PX+KisufDd37/D2yd6ufuGtRTmzG6bNxmfvGgpFy4v4evPH5xVnDtrvVEfuhCOsiR5wnfU209T12DCbPUgdOy6Eam4UNXUQ06Gg9PmxaY1xgfXlZKeJnMuOt7X0MkD245w0wY3l65cEKXo/KSlCf/24bPJz07n0z97fUark66BEQ6d6InrVg+SZ1BoMB91ns1NnKFYZUNIWZGqbu6OSjnMVBTnZXHZygU8+/oxRsdmVzM3OOLf5i3Iz+arV0dnmzeRBfnZ/PuH1/H2iV6++avIxyLuqffGbOjCdCSL67yyxktJXnysG9HClZtJYU6GEal4oKpUNfXEbKsX5MYNbtp6h9g+S8/Ud//wDu+09PIvHzorqtu8iVy0Yj6fvGgpj+04yv+LcKjEztqOmA1dmI6FBdmkp0lCu86D8/U2V9jfHzURT7Ez7jP4UlKkmrsH6RoYiZrTfCouXbmA4txMnto98y3f3oZOfrjtCB/e6ObSM6K7zZuML73/DM5eUsSXn9kfkQDsqovd0IXpcKQJi4uyEzon1eAd4HjXoK1bs0xFeXH8uyGkpEgFy2Hm0o0zEjIcaXxwfRm/rzpBR99wxF8X3OYtLIjdNm8imelp3H/zelThc0/snXaL6h+60BlXf1QofhtC4opUovmjQvEU+4e0xqrtz2SkpEhVBcph5tKNM1Ju3OBmZExn1B3z3t+/w+GWXu7+0FoKsmO3zZvIacVO/umGs9hT38G9v39nyvP2NnQyMqZxT5oHKStyJnTivLKmneLcTJYtSJx8VBBPSS4+hYY4brdTU6SaeigryomLAKxaXMCZZQU8HWFnhDeOdvDg9iP85cYlXLxifoyjO5Vrzy7lwxvdfP/lw/z58OQ923fVxnboQjjcrhxO9AzG9a95tEhEf1Qowckx8SyPCStSIvKwiLSIyIFpzrlERPaKyEER2TbhNYeIvCEiv4pGwNGguqk75vmoUG48x82BY93j3qypCN3m/cPVq+IU3al8/do1LC3J5fM/30t779Apr++s88Z06EI4ylw5qEJzV/RazsSLxo4BjnUOJORWDxifZhNPr1QkK6mfAFdO9aKIFAE/AK5V1TXATRNO+RxQNcv4os7gyBg1bX0xz0eFct26MjIc4T1T//G7tznS2se/xnmbNxFnZjrfu+UcOgdG+Jun3t3WZXTMF/OhC+FwJ3BfqdcSOB8FMC83k/ysdHutpFR1O+Cd5pRbgGdV9Wjg/PHWjyLiBq4CHppjnFHjcEsvYz6Nuf0gFFduJu9dtZBfvHGMkSkS0q8f7eC/Xqlh66YlXGTBNm8iqxYX8NWrVvHSoVYe/lPt+PHq5h76hscsS5oDuAODQhOx0HhHjZd5uZksT8B8FPgL1D0ludTG0YYQjZzUCsAlIi+LyB4R+VjIa/cCfweETR6IyCdEZLeI7G5tjU0vJjhZDhOPpHkoN2100943zEvVp7bvDW7zFhfm8JW/sG6bN5GPbinn/asX8q8vVvNmo38ashVFxRNZVJiNSGK6zitr2tlckZj5qCDlxU57raQiIB3YgH/FdAVwp4isEJGrgRZV3RPJRVT1QVXdqKob58+P3UqiurmH7Iy0cYt/vLho+XxK8rIm3fL9++/epqa1j7s/dBb5Fm7zJiIifPvGtczPy+Izj79O79Bo3IYuTEdmehqLCrITzobQ4O1P6HxUEE9xLo0dA1PuCqJNNESqEXhRVftUtQ3YDpwNXABcKyJ1wBPAZSLyaBTuNyeqm7s5Y2E+jhiVw0xFuiONG84p44/VLbSFJKP31Ae3eadx4XLrt3kTKXJmcu/N6znq7eer//Nm3IYuhKOsKIdjnYmVkwr6o+w+dCEc5cVOxnwatz8S0RCp54ALRSRdRJzAZqBKVf9eVd2q6gFuBv6oqrdG4X6zJlgOE8+keSg3bnAz6lOe2+vvMzU4MsbfPrWP0sIcvvIXKy2JKRI2Vczjc5ev4Bd7j8dt6EI43K7E69C5o9aLy5nBigXxTTVEm+B8wNo4bfkisSA8DrwGnCEijSLycRG5XURuB1DVKuBFYD+wE3hIVae0K1hJa88Q3r7huOejgqxYmM/Z7sLxLd+//fYQNW3+p3l22uZNxqcvWzbe5jYeQxfCUebKoblrcNbF21ZQGajXi1VRe7wY90rFaXJM2Obeqro1gnPuAe6Z5vWXgZdnElgsqAqOVI/jk72J3LhxCXf+4gCPvFbHQ6/Wcsvm03jP8hLL4okUR5rwn7du4NXDbSyzwUqgrMjJqE850TM03hnBzjR4+2nsGOD/vKfC6lDmTEleJrmZjrgNZUgpx/n4k70YdOOMlGvXlpLpSOPO5w4Gtnn2eZoXjnm5mVx7dqnVYQAh460SZMu3I/BUdHOCJ83B/0AlnkMZUkqkqpu6WVyYPeVI8nhQ6MzgfWv8Pcq/fePaqE6qSSVOduhMjOT5jpp2ipwZnLHQ+lVoNKgoyTUrqVhQ3dxj6SoqyJ1XrebH/+tcLlhm/22eXQlu8Rq9ibGSqqz1+6MSPR8VpLzYSYO3Py45wZQRqeFRH4dbei3NRwVZVJgd9VbAqUZ2hoOSvKyE6Ct1rHOABu8AmysSf6sXxFOcy6hPOd4Z+/rJlBGpwy29jPo0JiOsDNZQliA2hB0JXq83GeNDGeKQl0oZkQqOVI/FCCuDNbhdOQmxkqqsaacwJ8MWqYZoEfRKGZGKItXNPWSmp41/cw2Jj7vIL1KhXRrsSGWNN6nyUQDz87PIyXDEpWVLyohUVVM3KxbmkR7HGXGG2OJ25TA86ntXmZHdON45wFFvf1JYD0Lx2xDiU2icMr+xVpbDGGJD0IZg55YtO2qD+SjrS4mijSdOQxlSQqRae4Zo6x2yxZM9Q/QoK7L/oNDKI14KczJYlYR/ID0luTR4BxiL8XY7JUTqULAcJokSl4YQQ6edRaq2nU1Jlo8K4il2Mjzm43iMV7IpIVInG90l31+zVCYvK50iZ4ZtXedNXQPUt/ePF2YnGyeHMsT2+58aItXczcKCLOZZNDjAEDvs3LJlR42/Xi+Z/FGheEri45VKCZGqNknzpMXOg0Ira9opyE5P2lzowvxssjPSqItxy5akF6mRMX85jFU9pAyxJTgoVNV+XqnKGn8+Kt5dYONFWppQPi/2hcZJL1I1rX0Mj/mS8umKwb/dGxgZo6N/xOpQ3kVz1yB17f1Ju9ULEg+vVNKL1Hg5TJIuuVMduz7hO+mPSm6R8pTkUu/tj6nrP+lFqqqphwyHsHS+KYdJRuw6KLSypp38JM5HBfEU5zI86qOpO3bdEFJApLpZtiCfDFMOk5QEB4XardC4ssbLJk/y5qOCBMeux7LfedL/5lY3d7PKJM2TloKcdPKy0m1lQzjRPUhtW1/Sb/UAyse7IcRuJZvUIuXtG+ZE95BJmicxImI7r1RlEvaPmorFBdlkpqfFNHme1CJVbdFIdUN8KSuyV1+pyhov+VnprC5N/j+OfhuCM6aGzqQWKTuMsDLEHv9Kyj6J8x017ZybxP6oiZQX58a0r1RSi1R1UzcleVmU5GVZHYohhpS5cugZHKVrwHqvVEv3IDVtfUnZmmUqPMVO6r19MbMhJLVIVZmkeUoQbNliB69UZW1y1+tNRnlJLoMjPlp6YtN8MGlFanTMx9snepOqr7RhcsYHhdogL1VZ005eVjqrUyjFUFEc237nSStSde19DI/6TD4qBTjpOrc+L1VZ0865HldKtakenxwTI69U2O+kiDwsIi0icmCacy4Rkb0iclBEtgWOLRGRl0SkKnD8c9EMPBxVTf6kuel+kPwU52aSnZFmuQ2hpWeQmtbU8EeFUlqUQ4ZDYuaVikTufwJcOdWLIlIE/AC4VlXXADcFXhoFvqSqq4AtwP8VkdVzinYGVDV1k54mnL7AlMMkOyJiCxtCsH9Usg1dCIcjTVgyL3aFxmFFSlW3A95pTrkFeFZVjwbObwn8t0lVXw983gNUAWVzjjhCqpt7WLYgj6x0R7xuabCQMpfT8pVUZU07uZkOzkwBf9REKopj17IlGhvnFYBLRF4WkT0i8rGJJ4iIB1gP7JjqIiLyCRHZLSK7W1tb5xxUdVO3SZqnEHYYFLqj1su5FfNSKh8VpLw4l/r2vpj09YrGdzMd2ABcBVwB3CkiK4Ivikge8AzweVXtnuoiqvqgqm5U1Y3z58+fU0Cd/cMc7xo0Pc1TiLKiHLx9w/QPj1py/9aeIQ639KZcPiqIp8RJ//AYrTGwIURDpBqBF1W1T1XbgO3A2QAikoFfoB5T1WejcK+IqDZO85TDbXFfqWD/qGQduhCO8uLYFRpHQ6SeAy4UkXQRcQKbgSoREeBHQJWq/nsU7hMxwZo9M8IqdXBbPCh0PB9VVmjJ/a0mll6p9HAniMjjwCVAiYg0AncBGQCq+oCqVonIi8B+wAc8pKoHROQ9wEeBN0Vkb+ByX1HVF6L+LiZQ3dzDvNxM5uebcphUwepBoTtqvGz0zEvZvmWlRdmkp0lMvFJhRUpVt0Zwzj3APROOvQpYUmFZFUia+xdzhlRgQX4WGQ6xZLvX1jvEOy29XH9O3B5e2450R1rAhmDP7Z6tGPMph070mHxUipGWJpRa5JVK9vl6kVJeHJuWLUknUvXtfQyO+Iz9IAWxqmVLZU07zkwHZ6VoPiqIpziX+vb+qNsQkk6kguUwZiWVelg1KHRHbXtK56OCeIqd9A6N0tY7HNXrJt13tbq5G0easGxBntWhGOJMWZGTlp4hBkfG4nbPtt4h3j7Rm7LWg1CC/c6jXR6TdCJV1dTD0pJcsjNMOUyqEbQhNHXFbrzSRJ7fexyAi5bPzYCcDHhi5JVKOpGqbu42TvMUJd6DQkfGfPzo1Vo2eeZxlju181Hg/yPhSBOzkpqO7sERGjsGTDfOFCXeg0J/ue84xzoHuP2SpXG5n93JcKThduVQG2WvVFKJ1KFgOYzpIZWSLCrIxpEmcbEhqCo/3FbDioV5XLJiQczvlyiUB57wRZOkEikzwiq1SXeksaggOy6u85cOtXDoRA+fvOh00lJkKkwkeAJeqWjaEJJKpN5q6qEwJ4NFBdlWh2KwiDJXfGwID2yrobQwm2vXlcb8XolEeXEuPYOjdPRHb3JPUolUcKS6KYdJXdxxcJ2/frSDnbVePn7h0pT3Rk2kosRfQxnNvFTSfId9PuVQc4/paZ7iuF05NHUNMDLmi9k9Hnj5CIU5Gdx87pKY3SNRCbZsieYTvqQRqYaOfvqHx8yTvRSnzJWDT6E5Rl6pwy29/K7qBLedV05uVtj6/JTD7cohTaLrlUoakaoK9pAyHqmUJtYtWx7cfoRMRxq3ne+JyfUTnax0B6VFOWYlNRlVTT2kCSxfYFZSqUwsB4U2dw3yP28c48Mbl1CcZ3qVTUVFSW5U+0oljUhVN3fjKcklJ9OUw6Qyi4v8T3Zj8YTv4T/VMuZT/vpCY96cDn/LFrPdO4WqJtNDyuDfbiwsyIq667xrYISf7TjKVWtLOS0wsdcwOZ7iXLoGRujsj043hKQQqd6hUY56+01PcwNATAaFPrajnt6hUT55kVlFhSPaQxmSQqSC5TDGfmCA6A8KHRwZ4+FX67hweUnKDlqYCUGvVLTyUkkhUtXNgSd7KTg51nAqQa+Uzxed0oxnXz9GW+8Qn7r49KhcL9lxu5yUFmZHzauWFEaPqqZu8rPTKS005TAG/3ZvZExp6Rli0Rx/JsZ8yoPbj7DWXch5p6d2D/NIyc5w8Oe/vzxq10uOlVRTD6sWFZhyGAMQ3ZYtvznYTF17P7dffLr5+bKIhBcpVaW6ucd0PjCMEy2vlKrywLYjeIqdXLFmUTRCM8yChBepxo4BeodGjf3AME5pUXAlNTeReu1IO/sbu/jri5biMO1YLCPhRSpYDmNGWBmCODPTKc7NnLNI/ee2I5TkZfGhc9xRiswwGxJepKqbexCBM4xIGUIoc83NK3XweBevvNPGX13gMUM9LCYJRKobT3EuzsykeFBpiBJzHRT6w2015GWlc+uW8ihGZZgNYUVKRB4WkRYROTDNOZeIyF4ROSgi20KOXykih0TksIjcEa2gQ6lq6jFbPcMplBXlcLxzYFZtbBu8/fxq/3Fu2XwahTkZMYjOMBMiWUn9BLhyqhdFpAj4AXCtqq4BbgocdwDfBz4ArAa2isjqOcb7LvqHR6lr7zNOc8MplBXlMDjio71v5vVj//VKDY404X9fUBGDyAwzJaxIqep2wDvNKbcAz6rq0cD5LYHjm4DDqlqjqsPAE8B1c4z3Xbx9ohdVTKM7wym4XbPrK9XeO8STuxv44LqyORtBDdEhGjmpFYBLRF4WkT0i8rHA8TKgIeS8xsCxSRGRT4jIbhHZ3draGtGNTaM7w1TMdlDoT/9cx+CIj09ebAqJ7UI0ss3pwAbgciAHeE1EKoHJjCVTJghU9UHgQYCNGzdGlEiobuomLyudsoAvxmAIUjYL13nf0Cg/fa2e961eyDLTPNE2REOkGoE2Ve0D+kRkO3B24Hhop3o3cDwK9xtnUWEOHzhzkZl7ZjiFguwMCrLTZ2RDeGJXA10DI9xuColtRTRE6jngeyKSDmQCm4H/AKqB5SJSARwDbsafv4oan7rE/DAZpqbM5Yx4uzcy5uNHr9SwyTOPDeWuGEdmmAlhRUpEHgcuAUpEpBG4C8gAUNUHVLVKRF4E9gM+4CFVPRD42k8DvwEcwMOqejAm78JgmAS3K4ejETZe++W+4xzvGuRb158Z46gMMyWsSKnq1gjOuQe4Z5LjLwAvzC40g2FulBXl8NqRdlR12g4GqsoPt9VwxsJ8Lj1jQRwjNERCwjvODYapcLty6B0apWtg+pHfLx1q4dCJHj558VLTjsWGGJEyJC0n+0pNn5d64OUaSguzuebs0niEZZghRqQMSUtwUOh0T/j21Hews87Lxy9cSobD/DrYEfN/xZC0RLKSemDbEQpzMrj53CVTnmOwFiNShqSlyJmBM9MxpQ3hcEsPv3vrBLedV05ulumiYVeMSBmSFhGhrGjqli0/3FZDdkYat53viW9ghhlhRMqQ1LinaH7X3DXIL/Ye48Mbl1Ccl2VBZIZIMSJlSGqm6tD58J9qGfMpf32hKSS2O0akDEmN2+Wks3+E3qHR8WNdAyP8bMdRrlpbypJ5TgujM0SCESlDUhPskBGaPH+0sp7eoVE+eZFZRSUCRqQMSc3Eli2DI2P8+E91XLi8hDPLCq0MzRAhRqQMSc3EQaHPvN5IW+8QnzLtWBIGI1KGpKYkN4vM9DSOdQww5lP+a3sNa92FnHd6sdWhGSLEiJQhqUlLE9xFOTR2DPDigWbq2vu5/eLTTSFxAmFEypD0lAVm8D2w7QgVJblcsWaR1SEZZoCpBTAkPWVFOfzpcBs+hX++/iwcpt10QmFWUoakx+3KwadQkpfFDedMObDIYFOMSBmSnqAN4a8u8JCd4bA4GsNMMSJlSHouWbGAj7+nwhQSJygmJ2VIely5mdx59WqrwzDMErOSMhgMtsaIlMFgsDVGpAwGg60xImUwGGyNESmDwWBrjEgZDAZbY0TKYDDYGiNSBoPB1oiqWh3DKYhIK1Af4eklQFsMw4kVJu74kqhxQ+LGPpO4y1V1/mQv2FKkZoKI7FbVjVbHMVNM3PElUeOGxI09WnGb7Z7BYLA1RqQMBoOtSQaRetDqAGaJiTu+JGrckLixRyXuhM9JGQyG5CYZVlIGgyGJMSJlMBhsTUKLlIhcKSKHROSwiNxhdTyRICJLROQlEakSkYMi8jmrY5oJIuIQkTdE5FdWxxIpIlIkIk+LSHXg+36e1TFFgoh8IfAzckBEHheRbKtjmgwReVhEWkTkQMixeSLyOxF5J/Bf12yvn7AiJSIO4PvAB4DVwFYRSYT2i6PAl1R1FbAF+L8JEneQzwFVVgcxQ74LvKiqK4GzSYD4RaQM+CywUVXPBBzAzdZGNSU/Aa6ccOwO4A+quhz4Q+DfsyJhRQrYBBxW1RpVHQaeAK6zOKawqGqTqr4e+LwH/y9MQowwERE3cBXwkNWxRIqIFAAXAT8CUNVhVe20NKjISQdyRCQdcALHLY5nUlR1O+CdcPg64KeBz38KfHC2109kkSoDGkL+3UiC/LIHEREPsB7YYXEokXIv8HeAz+I4ZsJSoBX4cWCb+pCI5FodVDhU9RjwHeAo0AR0qepvrY1qRixU1Sbw/2EGFsz2QoksUpNNeEwYP4WI5AHPAJ9X1W6r4wmHiFwNtKjqHqtjmSHpwDnAf6rqeqCPOWw94kUgh3MdUAGUArkicqu1UVlDIotUI7Ak5N9ubLocnoiIZOAXqMdU9Vmr44mQC4BrRaQO/9b6MhF51NqQIqIRaFTV4Gr1afyiZXfeC9SqaquqjgDPAudbHNNMOCEiiwEC/22Z7YUSWaR2ActFpEJEMvEnFZ+3OKawiIjgz49Uqeq/Wx1PpKjq36uqW1U9+L/Xf1RV2/9lV9VmoEFEzggcuhx4y8KQIuUosEVEnIGfmctJgIR/CM8DtwU+vw14brYXSti5e6o6KiKfBn6D/8nHw6p60OKwIuEC4KPAmyKyN3DsK6r6gnUhJT2fAR4L/DGrAf7K4njCoqo7RORp4HX8T4TfwKblMSLyOHAJUCIijcBdwN3AkyLycfyCe9Osr2/KYgwGg51J5O2ewWBIAYxIGQwGW2NEymAw2BojUgaDwdYYkTIYDLbGiJQh6ojImIjsDfmImsNbRDyh1faG5CdhfVIGWzOgquusDsKQHJiVlCFuiEidiPyriOwMfCwLHC8XkT+IyP7Af08LHF8oIv8jIvsCH8GyEIeI/Feg19JvRSTHsjdliDlGpAyxIGfCdu8vQ17rVtVNwPfwd1Ug8Pl/q+pa4DHgvsDx+4Btqno2/nq7YEXBcuD7qroG6AQ+FNN3Y7AU4zg3RB0R6VXVvEmO1wGXqWpNoMi6WVWLRaQNWKyqI4HjTapaEphk7VbVoZBreIDfBZqpISJfBjJU9VtxeGsGCzArKUO80Sk+n+qcyRgK+XwMk1tNaoxIGeLNX4b897XA53/mZGvcjwCvBj7/A/ApGO+tXhCvIA32wfwFMsSCnJAOD+DvLx60IWSJyA78fyC3Bo59FnhYRP4WfxfNYJeCzwEPBirpx/ALVlOsgzfYC5OTMsSNQE5qo6q2WR2LIXEw2z2DwWBrzErKYDDYGrOSMhgMtsaIlMFgsDVGpAwGg60xImUwGGyNESmDwWBr/j+ySmt9D+4BkQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "all_loss=torch.tensor(all_loss)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_loss.detach().numpy(), label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_accuracy, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for evaluation of the model over the dataset, here it is charged the test dataset as It was doing eiht the train dataset. Then the data are iterating and processed into the model to see if the prediciton match the actual labels, the code is way similar to the one used in the training phase except for the part of optimizer/backward ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.RandomRotation(15),     \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    \n",
    "    transforms.RandomResizedCrop(16),\n",
    "    transforms.RandomResizedCrop(4),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomAffine(degrees=4, translate=(0.4, 0.1)),\n",
    "    transforms.ToTensor(),     \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = CustomCIFAR2(\n",
    "    root='./cifar-10-batches-py', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "model = Sequencer2DModel(num_classes=5, in_channels=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_accuracy=[]\n",
    "c=0\n",
    "\n",
    "total_correct=0\n",
    "total_samples=0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        \n",
    "        c+=1\n",
    "        print(c)\n",
    "        if c==78:\n",
    "            continue\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "       \n",
    "        \n",
    "        outputs = model(inputs)\n",
    "\n",
    "        \n",
    "        \n",
    "        predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        predicted_labels = predicted_labels.argmax(dim=1)\n",
    "        predicted_labels= predicted_labels.argmax(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        correct = (predicted_labels[:,0] == labels).sum().item()\n",
    "        \n",
    "        total_correct += correct\n",
    "        total_samples += len(labels)\n",
    "        \n",
    "        accuracy = (total_correct / total_samples) * 100.0  \n",
    "        \n",
    "        \n",
    "        all_accuracy.append(accuracy)\n",
    "        for label in labels:\n",
    "            all_labels.append(labels)\n",
    "        \n",
    "        \n",
    "        print(f\"batch: [{c}/{len(data_loader)}]\",f\"Test Accuracy: {accuracy :.2f}%\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code used to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "model_pkl_file = \"./sequencer_model384S.pkl\"\n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
