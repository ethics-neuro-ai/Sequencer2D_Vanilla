{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTMVertical:\n",
    "This is a module that implements a Bidirectional Long Short-Term Memory (BiLSTM) layer with a vertical direction of computation.\n",
    "It takes two arguments: input_size and hidden_size, which determine the input and hidden state dimensions of the LSTM.\n",
    "Inside the __init__ method, it initializes an nn.LSTM module with the specified input and hidden sizes. It's set to be bidirectional (bidirectional=True) and accepts input data in batches (batch_first=True).\n",
    "In the forward method, it applies the BiLSTM to the input tensor x and returns the output.\n",
    "BiLSTMHorizontal:\n",
    "This is another module similar to BiLSTMVertical, but it implements a BiLSTM layer with a horizontal direction of computation.\n",
    "It has the same constructor and forward method structure as BiLSTMVertical.\n",
    "FullyConnectedLayer:\n",
    "This module represents a fully connected (linear) layer in a neural network.\n",
    "It takes two arguments: input_size and output_size, determining the input and output dimensions of the linear layer.\n",
    "In the __init__ method, it initializes an nn.Linear module with the specified input and output sizes.\n",
    "In the forward method, it applies the linear transformation to the input tensor x and returns the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "class BiLSTMVertical(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTMVertical, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class BiLSTMHorizontal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BiLSTMHorizontal, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        output, _ = self.lstm(x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "       \n",
    "        # Apply He initialization to linear layer weights\n",
    "        init.kaiming_normal_(self.fc.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2D\n",
    "Is a sequencer in which the input data (images) are splitted into vertical adn horiziontal component, then vertical data will be passed as input to BilstmVertical ,while horizontal_data will be passed to BiLSTMHorizontal, to do so the input data are first processed in such a way we'll have a tensor with vertical data of each image and another tensor with horizontal data of each image. Oncee we get the outputs from both BiLSTMvertical and BiLSTMHorizontal, we 'll concatenate them and pass as input the result to Fully Connected Layer . At the end of the sequencer the results will be a list of tensor ( each tensor has inside data belongs to each image processed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sequencer2D(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, mlp_input_size, mlp_output_size):\n",
    "        super(Sequencer2D, self).__init__()\n",
    "\n",
    "       \n",
    "        vertical_bilstm = BiLSTMVertical(input_size, hidden_size)\n",
    "        horizontal_bilstm = BiLSTMHorizontal(input_size, hidden_size)\n",
    "        fully_connected = FullyConnectedLayer( mlp_input_size,mlp_output_size)  \n",
    "\n",
    "        # Move models to GPU if available\n",
    "        #vertical_bilstm.to(device)\n",
    "        #horizontal_bilstm.to(device)\n",
    "        #fully_connected.to(device)\n",
    "\n",
    "        self.vertical_bilstm = vertical_bilstm\n",
    "        self.horizontal_bilstm = horizontal_bilstm\n",
    "        self.fully_connected = fully_connected\n",
    "        \n",
    "       # Batch normalization layers\n",
    "        self.bn_vertical = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.bn_horizontal = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.bn_fc = nn.BatchNorm1d(mlp_output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x)\n",
    "        if len(x.shape)==4:\n",
    "            batch_size,channel, height, width = x.size()\n",
    "            x=x.squeeze(1)\n",
    "        elif len(x.shape) == 3 :\n",
    "            batch_size, height, width = x.size()\n",
    "        output=[]\n",
    "        for index in range(batch_size):\n",
    "            \n",
    "            x_vertical = x[index][:, :height//2]  #split the image vertically\n",
    "            x_horizontal = x[index][ :,width//2:]   #split image horizontally\n",
    "\n",
    "            vertical_data = x_vertical.permute( 1,0).clone() #permutation to fit bilstm\n",
    "            horizontal_data = x_horizontal.permute( 1,0).clone()#same here\n",
    "            \n",
    "            \n",
    "            \n",
    "            H_ver = self.vertical_bilstm(vertical_data)\n",
    "            H_hor = self.horizontal_bilstm(horizontal_data)\n",
    "            \n",
    "            \n",
    "            H_ver = H_ver.permute(1,0).clone() #permute to fit torch.cat\n",
    "            H_hor = H_hor.permute(1,0).clone()\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            H_concat = torch.cat((H_ver, H_hor), dim=1)#concatenaiton of the horizontal and vertical bilstm outputs\n",
    "            H_concat = H_concat.unsqueeze(0) #set dimension to fit fully conncted layer\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            output_pieces = self.fully_connected(H_concat)\n",
    "            \n",
    "            #output_pieces=torch.tensor(output_pieces, dtype=torch.float32)\n",
    "            output.append(output_pieces) #add output to lists of each image data processed here\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Embedding : this costum function given an image , a patch size, will splitted the image in image_size/patch_size pieces.\n",
    "\n",
    "image_size: The size of the input image. It represents both the height and width of the image.\n",
    "patch_size: The size of each non-overlapping patch that the image is divided into.\n",
    "in_channels: The number of input channels in the image. For RGB images, this is typically 3 (for the red, green, and blue channels).\n",
    "embed_dim: The dimension of the embedded representations for each patch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        \n",
    "        self.num_patches = (image_size // patch_size) ** 2 #compute the number of patches in such a way they won't overlap\n",
    "\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size) #convolutional layer takes the input image, divides it into patches, and embeds each patch into a lower-dimensional representation.\n",
    "        self.bn = nn.BatchNorm2d(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.reshape(B, self.num_patches, C, -1).permute(0, 2, 1, 3).clone()\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def output_dimension(self):\n",
    "        return self.embed_dim * self.num_patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PW Linear Layer compute  pointwise linear transformations on a list of input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(PWLinearLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        \n",
    "    def forward(self, input_list):\n",
    "        # output_list = []\n",
    "        # for input_tensor in input_list:\n",
    "        #     output_tensor = self.linear(input_tensor)\n",
    "        #     output_list.append(output_tensor)\n",
    "        # stacked_output = torch.stack(output_list, dim=0)\n",
    "        stacked_output = torch.cat(input_list, dim=0)\n",
    "        # Apply linear transformation\n",
    "        output_tensor = self.linear(stacked_output)\n",
    "        return output_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch merging, take the images splitted in patches (for examples by patch embedding ) and merge it together, the number of patches merged is based on the output channlee and  the  scale factor number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "       \n",
    "\n",
    "    def forward(self, patch_list):\n",
    "        \n",
    "        x = torch.cat(patch_list, dim=1).permute(1,2,0).clone() # compination of the list of patches into a single tensor + peermutation to fit convolutional layer\n",
    "        \n",
    "        \n",
    "        x = self.conv(x) # Apply to the input x, a 1x1 convolution to merge the patches together\n",
    "        \n",
    "        \n",
    "        x = nn.functional.interpolate(x, scale_factor=self.scale_factor, mode='nearest') #resize the feature map using a nearest-neighbor upsampling\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAveragePooling, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.mean(x, dim=(-2, -1))# perform global average pooling along spatial dimensions using the mean\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencer2Dmodel-> Is an architecture that resambles vision transformer except for the fact that Sequencer2D model is using BiLSTM2D (splitting image in horizzontal and vertical data) instead of self attention. It started with patch embedding layeer so the images will be divided in 8x8 patches, than ,after a normalization, there is the first sequence block of 4 Sequencer , the output of this block converge in PatchMerging, where the patches are resembled together ,then there is a second sequencer block made of 3 sequencer2d, followed by pointwise linear layer,then other sequencer block with 8 sequencers2D, then other point wise linear layer and  the final sequencerBlock with 3 sequencer2D layer and at the end there are linears+ global avarage pooling layers in order to make the output dimension as expected-\n",
    "\n",
    "The parameters are the one suggested by the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomCifar2, this custom function process imace into the dataset in order to take only the first five classes, I've tried this apporch because each batch processing was too slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CustomCifar2, this custom function process imace into the dataset in order to take only the first five classes, I've tried this apporch because each batch processing was too slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Sequencer2DModel(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super(Sequencer2DModel, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "\n",
    "       \n",
    "        self.patch_embedding_1 = PatchEmbedding(32, 8, in_channels, 16)#  patch embedding with an 8x8 kernel size for each patch\n",
    "        self.ln_1 = nn.LayerNorm(16)\n",
    "\n",
    "       \n",
    "        self.sequencer_block_1 =  nn.Sequential(\n",
    "            Sequencer2D(16, 48, 16, 96),\n",
    "            Sequencer2D(96, 96, 96, 192),\n",
    "            Sequencer2D(192, 192, 192,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "\n",
    "\n",
    "       \n",
    "        self.patch_merging=PatchMerging( 49152,128,2)#it still depend on batch size!!! it is sooo wrong\n",
    "\n",
    "      \n",
    "        self.sequencer_block_2 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 3,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "\n",
    "       \n",
    "        self.pw_linear_1 = PWLinearLayer( 384,384)\n",
    "        \n",
    "        \n",
    "        self.sequencer_block_3 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            \n",
    "            \n",
    "           \n",
    "        )\n",
    "\n",
    "        self.pw_linear_2 = PWLinearLayer(384, 384)\n",
    "\n",
    "        \n",
    "        self.sequencer_block_4 =  nn.Sequential(\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384),\n",
    "            Sequencer2D(384, 192, 384,384)\n",
    "        )\n",
    "        self.pw_linear_3 = PWLinearLayer(384, 384)\n",
    "\n",
    "\n",
    "        \n",
    "        self.ln_2 = nn.LayerNorm(384)\n",
    "\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 384))\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(384, num_classes)\n",
    "        \n",
    "        \n",
    "        # Apply He initialization to linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.permute(0,3,1,2).clone()  \n",
    "        #x = x.to(torch.float32).permute(0,3,1,2)  \n",
    "        x = self.patch_embedding_1(x)\n",
    "        x = self.ln_1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.sequencer_block_1(x)\n",
    "       \n",
    "        print(len(x))\n",
    "        \n",
    "        #x = self.patch_merging(x)\n",
    "        \n",
    "        \n",
    "        # x = self.sequencer_block_2(x)\n",
    "        \n",
    "        \n",
    "        x = self.pw_linear_1(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.sequencer_block_3(x)\n",
    "        \n",
    "        \n",
    "        # x = self.pw_linear_2(x)\n",
    "        # x = F.relu(x)\n",
    "    \n",
    "        # x = self.sequencer_block_4(x)\n",
    "       \n",
    "        # x = self.pw_linear_3(x)\n",
    "        # x = F.relu(x)\n",
    "        \n",
    "        x = self.ln_2(x)\n",
    "        \n",
    "        x = self.global_avg_pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "\n",
    "        #x = x.squeeze(-1).squeeze(-1)\n",
    "        #x=x[:,0]\n",
    "        #x=x[:,0]\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomCIFAR2(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(CustomCIFAR2, self).__init__()\n",
    "        self.cifar10 = datasets.CIFAR10(root, train=train, transform=transform, target_transform=target_transform, download=download)\n",
    "        \n",
    "        \n",
    "        self.keep_classes = [0, 1,2]  \n",
    "\n",
    "      \n",
    "        self.data, self.targets = self.filter_classes()\n",
    "\n",
    "    def filter_classes(self):\n",
    "        mask = np.isin(self.cifar10.targets, self.keep_classes)\n",
    "        data = [self.cifar10.data[i] for i, include in enumerate(mask) if include]\n",
    "        targets = [self.cifar10.targets[i] for i, include in enumerate(mask) if include]\n",
    "        return data, targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model, with trnsform.Compose function that helps me with preprocessing and data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Sequencer2DModel(num_classes=3, in_channels=3)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "    transforms.RandomHorizontalFlip(),   # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(15),      # Randomly rotate the image by up to 15 degrees\n",
    "    \n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),# Adjust brightness, contrast, saturation, and hue\n",
    "    transforms.RandomResizedCrop(16),\n",
    "    transforms.RandomResizedCrop(4),# Randomly crop and resize the image to 224x224\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomAffine(degrees=90, translate=(0.4, 0.1)),# Randomly translate the image\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "#train_dataset = torchvision.datasets.CIFAR10(root='./cifar-10-batches-py', train=True, transform=transform)\n",
    "custom_dataset = CustomCIFAR2(root='./cifar-10-batches-py', train=True, transform=transform, download=True)\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part the code aims to train the model: it iterates over  the number of epochs and then over all the batchs in the datasets takes the images and the labels and pass it to the model ,get the model outputs, and reshape it in order to make it fit well the loss function(CrossEntropyLoss function , 'cause it is a multiclassification problem), then performs backward step, and calls the optimizer (Adam starting with 0.01 learning rate) ,then it takes the prediction and compere it with the true labels,after that the accuracy is computed simply by divided the true presiction to the total  number of images processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [0/118] Loss: 4.5098 Accuracy: 28.91%\n",
      "128\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Epoch [1/10] Batch [1/118] Loss: 67.7880 Accuracy: 34.38%\n",
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [2/118] Loss: 105.7848 Accuracy: 27.34%\n",
      "128\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch [1/10] Batch [3/118] Loss: 59.6171 Accuracy: 29.69%\n",
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [4/118] Loss: 23.4722 Accuracy: 35.16%\n",
      "128\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch [1/10] Batch [5/118] Loss: 17.7311 Accuracy: 39.06%\n",
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [6/118] Loss: 12.4787 Accuracy: 29.69%\n",
      "128\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch [1/10] Batch [7/118] Loss: 10.2111 Accuracy: 33.59%\n",
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [8/118] Loss: 3.6032 Accuracy: 32.81%\n",
      "128\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Epoch [1/10] Batch [9/118] Loss: 7.3485 Accuracy: 25.78%\n",
      "128\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Epoch [1/10] Batch [10/118] Loss: 2.7931 Accuracy: 39.06%\n",
      "128\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch [1/10] Batch [11/118] Loss: 1.8266 Accuracy: 31.25%\n",
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [12/118] Loss: 1.3658 Accuracy: 36.72%\n",
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [13/118] Loss: 1.3485 Accuracy: 37.50%\n",
      "128\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch [1/10] Batch [14/118] Loss: 1.1968 Accuracy: 28.12%\n",
      "128\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Epoch [1/10] Batch [15/118] Loss: 1.1103 Accuracy: 28.12%\n",
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [16/118] Loss: 1.1501 Accuracy: 36.72%\n",
      "128\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "Epoch [1/10] Batch [17/118] Loss: 1.1061 Accuracy: 29.69%\n",
      "128\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Epoch [1/10] Batch [18/118] Loss: 1.1398 Accuracy: 28.12%\n",
      "128\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Epoch [1/10] Batch [19/118] Loss: 1.1149 Accuracy: 32.81%\n",
      "128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ef0c772ccdc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "all_accuracy=[]\n",
    "all_loss=[]\n",
    "# beginning of training\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        running_loss = 0.0  # take track of the loss \n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            if i==195:\n",
    "                continue\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs=inputs.to(torch.float32)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            all_loss.append(loss)\n",
    "            \n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print or log gradients\n",
    "            # print(\"Gradients (max/min/mean):\")\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(f\"{name}: {param.grad.max().item():.6f} / {param.grad.min().item():.6f} / {param.grad.mean().item():.6f}\")\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted_probabilities = outputs.argmax(dim=1)\n",
    "            print(predicted_probabilities)\n",
    "            correct = (predicted_probabilities == labels).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            batch_accuracy = (correct / labels.size(0)) * 100.0\n",
    "            all_accuracy.append(batch_accuracy)\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss: {loss.item():.4f} Accuracy: {batch_accuracy:.2f}%\")\n",
    "            if i % 200 == 199: \n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}] Batch [{i}/{len(data_loader)}] Loss: {loss.item():.4f} Accuracy: {batch_accuracy:.2f}%\")\n",
    "                print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}\")\n",
    "                running_loss = 0.0\n",
    "            torch.cuda.empty_cache()\n",
    "        epoch_accuracy = (total_correct / total_samples) * 100.0\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                # Add any other relevant information\n",
    "            }, 'checkpoint.pth')\n",
    "            \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code used to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "model_pkl_file = \"sequencer_model384M.pkl\"\n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for evaluation of the model over the dataset, here it is charged the test dataset as It was doing eiht the train dataset. Then the data are iterating and processed into the model to see if the prediciton match the actual labels, the code is way similar to the one used in the training phase except for the part of optimizer/backward ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "all_loss=torch.tensor(all_loss)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_loss.detach().numpy(), label='Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_accuracy, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy Over Time')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
